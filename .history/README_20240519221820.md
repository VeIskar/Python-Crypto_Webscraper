# Crypto web scraping and analysis

This is a Beautifulsoup data scraping project designed to scrape cryptocurrency data from CoinGecko and perform analysis on it with the help of jupyter. The project comprises two main components:
- **scraper.py** file which contains python code used for data scraping
- **analysis.ipynb** file which performs analysis and visualization on the scrapped data

## Features
**scraper.py**:
- Customizable page scraping: user can specify how many pages of cryptos data they want to scrape from CoinGecko up to the maximum number of pages available on the website.
- Updating the data: user is given an option to scrape the data every few minutes which helps tracking changes in real-time.
- Data storage: python sript checks the project path and creates a separate directory named stored_d in which scraped data is stored (in CSV format). Each file is timestamped to ensure uniqueness and maintain a history of data.

**analysis.ipynb**:
- Summary statistics: The notebook calculates and displays summary statistics for some of the metrics such as price, 24-hour volume, market cap, and fully diluted valuation (FDV).
- Scatter plot: visualizations include scatter plot of price versus market cap to identify trends and outliers.
- Histograms: notebook generates histograms for various data columns (FDV,percentages,prices etc.) to show it distribution.
- Line Plots: crypto prices are plotted over different price ranges to visualize trends across different categories of coins.
- Boxplots: boxplots are used to visualize the distribution of percentage changes (currently without separation between decrease and increase) over different time periods (1 hour, 24 hours), showing the spread, central tendency, and potential outliers in the data.

## Requirements
To run the project you would need:
- Python 3.8 or higher
- Jupyter Notebook
- Python libraries: ***requests***, ***beautifulsoup4***, ***pandas***, ***numpy***, ***seaborn***, ***matplotlib***

you can check all the requirements in requirements.file

## Setup
1. Clone the repository
2. Install the required dependencies 
3. Specify the input parameters in scraper.py after running (The script will scrape user specified number of pages and store them stored_d directory as CSV files. If updates are enabled, the script will continue to scrape and save data at the specified time intervals.)
4. Open Jupyter Notebook
5. Run the cells for further analysis (analysis file automatically gets the path of stored csv data)
